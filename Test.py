import pandas as pd
import numpy as np
import evaluate
from datasets import load_dataset
from transformers import GPT2Tokenizer
from transformers import GPT2ForSequenceClassification
from transformers import TrainingArguments, Trainer

print("ğŸ”¥ TrainingArguments from:", TrainingArguments.__module__)

# íŠ¸ìœ„í„° ë°ì´í„° ì…‹ì„ ë¶ˆëŸ¬ì˜¨ë‹¤
dataset = load_dataset("mteb/tweet_sentiment_extraction")

# íŠ¸ë ˆì´ë‹ ë°ì´í„° ì…‹ì„ ë°ì´íƒ€ í”„ë ˆì„ í˜•íƒœë¡œ ë³€í™˜í•œë‹¤ 
df = pd.DataFrame(dataset['train'])

# í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ì •í•œë‹¤
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# ì´ì „ì— ì„¤ì •í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•´ì„œ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# íŠ¸ìœ„í„° ë°ì´í„° ì…‹ì„ í† í¬ë‚˜ì´ì¦ˆí•œë‹¤ 
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# í† í¬ë‚˜ì´ì¦ˆ ëœ ë°ì´í„° ì…‹ì„ íŠ¸ë ˆì¸ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_test_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

# GPT2ë¥¼ 3 ì¢…ë¥˜ì˜ ë¼ë²¨ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“ ë‹¤
model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=3)

# í‰ê°€ ë©”íŠ¸ë¦­ì„ ë¶ˆëŸ¬ì˜¨ë‹¤
metric = evaluate.load("hyperml/balanced_accuracy")

# ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë¼ë²¨ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤ 
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
    
# íŠ¸ë ˆì´ë‹ì— í•„ìš”í•œ ì„¤ì •ì„ ì •ì˜í•œë‹¤
# TrainingArguments í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤
training_args = TrainingArguments(
    output_dir="test_trainer", # ëª¨ë¸ì„ ì €ì¥í•  ê³µê°„ 
    evaluation_strategy="epoch", # ì „ì²´ ë°ì´í„° ì…‹ì„ ëŒë¦¬ê³  í‰ê°€í•˜ëŠ” ì „ëµì„ íƒí•œë‹¤
    per_device_train_batch_size=1, # Reduce batch size here
    per_device_eval_batch_size=1, # Optionally, reduce for evaluation as well
    gradient_accumulation_steps=4 # 4 ë¬¸ì¥ ë§ˆë‹¤ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤ 
)

# íŠ¸ë ˆì´ë‹ í•˜ëŠ” í´ë ˆìŠ¤ë¥¼ ì´ìš©í•´ì„œ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤
trainer = Trainer(
    model=model, # 3 ì¢…ë¥˜ì˜ ë¼ë²¨ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“ ë‹¤
    args=training_args, # íŠ¸ë ˆì´ë‹ì— í•„ìš”í•œ ì„¤ì • Argumentsë¥¼ ì´ìš©í•´ì„œ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤
    train_dataset=small_train_dataset, # 
    eval_dataset=small_test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()
trainer.evaluate()